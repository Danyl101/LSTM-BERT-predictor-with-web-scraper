							MACHINE LEARNING DEBUG REPORT



Tags:[Mismatch,Loss,Variance]
__________________________________________________________________________
Error

return self.relu(x + res)
                     ~~^~~~~
RuntimeError: The size of tensor a (64) must match the size of tensor b (60) at non-singleton dimension 2

Reason and Solution 

 The conv layer(x) reduces the time steps presents which is created by dilation and kernel size , this is usually offset by padding which is this case was not

 enough , so we did a slight code snippet to reduce the time steps either from conv layer(x) or residual layer(res) depending on which was bigger

if x.size(2) > res.size(2):
    x = x[:, :, :res.size(2)]			x[B:C:T] where B =Batch Size  	
elif x.size(2) < res.size(2):  				       C= Channels
    res = res[:, :, :x.size(2)]                                T=Time Steps

						here x[B:C:T(begin):T(end)] basically sliced off the end portion of T 

_______________________________________________________________________________

Note [Loss]

Low Training Loss and High Validation Loss indicates that the model is either overfitting , or has poor generalization , in this case it was due to large difference between even standardised values present in training set , validation set , test set , which caused machine to not be able to predict validation test properly, this was mitigated by log transformation 

___________________________________________________________________________________

Note[Loss]

High Training Loss and Low Validation Loss indicates that the model has data leakage as in data is leaked from training to validation and test sets , occurs when you use same scaler and fit_transform multiple times with same scaler

___________________________________________________________________________________

Note [Loss,Variance]

If the variance between train , test and val values is what might be causing issues , then increasing dilation , kernel size for training in train dataset might make it help understand the patterns more since it can look at further and contextualise in a larger region

___________________________________________________________________________________

Error 

RuntimeError: expected padding to be a single integer value or a list of 1 values to match the convolution dimensions, but got padding=[1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4]

for i, out_channels in enumerate(num_channels):
            #Dilation increases the time steps checked by convoluion
            dilation = [1,2,3,4]
            layers.append(TCNBlock(in_channels, out_channels, kernel_size, dilation))
            in_channels = out_channels

self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size,
                               padding=(kernel_size - 1) * dilation,
                               dilation=dilation)

Reason and Solution

Since padding is generated by multiplying kernel sizes with dilation values , here since dilation is an array , the padding multiplied the array which caused the padding to contain multiple arrays instead of a single digit 

____________________________________________________________________________________________

Error 

Traceback (most recent call last):
  File "d:\Prediction Model\Models\lstm.py", line 148, in <module>
    test_preds, test_actuals, mse, rmse, mae, r2, mape = predict(model, test_loader)
                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Prediction Model\Models\lstm.py", line 143, in predict
    mse, rmse, mae, r2, mape = evaluate_metrics(preds, targets)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Prediction Model\Models\lstm.py", line 83, in evaluate_metrics
    y_true = y_true.detach().cpu().numpy()
             ^^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'detach'

        for x, y in loader:
            x, y = x.to(device), y.to(device)
            pred = model(x)
            preds.append(pred) #Appends models predictions to the list
            targets.append(y) #Appends actual values to the list

    mse, rmse, mae, r2, mape = evaluate_metrics(preds, targets)
    return np.concatenate(preds), np.concatenate(targets),mse,rmse,mae,r2,mape

    def evaluate_metrics(y_true, y_pred):
    y_true = y_true.detach().cpu().numpy()
    y_pred = y_pred.detach().cpu().numpy()


Reason & Solution

Torch user a special data type torch.Tensor for computation , but this datatype is only for torch based computations and not for general computations ,so when we use a general formula like rmse it cannot handle such data type , so we convert the data type beforehand

with torch.no_grad():
        for x, y in loader:
            x, y = x.to(device), y.to(device)
            pred = model(x)
            preds.append(pred) #Appends models predictions to the list
            targets.append(y) #Appends actual values to the list
            
    preds = torch.cat(preds, dim=0)
    targets = torch.cat(targets, dim=0)

    mse, rmse, mae, r2, mape = evaluate_metrics(preds, targets)
    return preds,targets,mse,rmse,mae,r2,mape




            